from torch import nn
import torch
import matplotlib.pyplot as plt
import datetime
import torchvision.transforms.functional as tF
import numpy as np


class ClampLayer(nn.Module):
    def __init__(self, min=None, max=None):
        super().__init__()
        self.min = min
        self.max = max
        self.kwargs = {}
        if min is not None:
            self.kwargs['min'] = min
        if max is not None:
            self.kwargs['max'] = max

    def forward(self, input):
        return torch.clamp(input, **self.kwargs)
    
def bernoulli_log_likelihood(x_true, x_distr):
    """
    Compute log-likelihood of objects x_true for the generated by model
    component-wise Bernoulli distributions.
    Each object from x_true has K corresponding distrbutions from x_distr.
    Log-likelihood estimation must be computed for each pair of an object
    and a corresponding to the object distribution.
    Do not forget about computational stability!
    Do not divide log-likelihood by the dimensionality of the space of objects.

    Input: x_true, Tensor of shape n x D.
    Input: x_distr, Tensor of shape n x K x D - parameters of component-wise
           Bernoulli distributions.
    Return: Tensor of shape n x K - log-likelihood for each pair of an object
            and a corresponding distribution.
    """
    # assert x_true.shape[0] == x_distr.shape[0], f"bad n dim: {x_true.shape[0]} != {x_distr.shape[0]}"
    # assert x_true.shape[1] == x_distr.shape[2], f"bad D dim: {x_true.shape[1]} != {x_distr.shape[2]}"

    x_true = torch.unsqueeze(x_true, 1)
    res = torch.log(1 - x_distr) * (x_true == 0) + torch.log(x_distr) * (x_true == 1)
    return res.sum(dim=2)

def reconstruction_log_likelihood(x_true, x_distr):
    """Compute some log-likelihood of x_true judging by x_distr
    Input: x_true, Tensor of shape n x D1 x D2 ... Dn
    Input: x_distr, Tensor of shape n x K x D1 x D2 ... Dn
    Return: Tensor of shape n x K - log-likelihood for each pair of an object
            and a corresponding distribution.
    """
    x_true = torch.unsqueeze(x_true, 1) # n k ds
    gaussian_log_pdf(x_true, sigma_)
    # somewhat normal distributions with some sigma, without constants
    res = -(x_true - x_distr) ** 2
    return res.sum(dim=list(range(2, len(x_distr.shape))))


def kl(q_mu, q_sigma, p_mu, p_sigma):
    """

    Compute KL-divergence KL(q || p) between n pairs of Gaussians
    with diagonal covariational matrices.
    Do not divide KL-divergence by the dimensionality of the latent space.

    Input: q_mu, p_mu, Tensor of shape n x d - mean vectors for n Gaussians.
    Input: q_sigma, p_sigma, Tensor of shape n x d - standard deviation
           vectors for n Gaussians.
    Return: Tensor of shape n - each component is KL-divergence between
            a corresponding pair of Gaussians.
    """
    # assert torch.all(q_sigma >= 0), f'negative values in q_sigma: {(q_sigma < 0).sum()} / {torch.numel(q_sigma)}'
    # assert torch.all(p_sigma >= 0), f'negative values in q_sigma: {(p_sigma < 0).sum()} / {torch.numel(p_sigma)}'
    kl_div = torch.log(p_sigma / q_sigma) + (q_sigma ** 2 + (q_mu - p_mu) ** 2) / (2 * p_sigma**2) - 0.5
    return kl_div.sum(dim=1)


def pretty_now():
    return str(datetime.datetime.now(datetime.timezone(datetime.timedelta(hours=3))).strftime("%Y-%m-%d_%H:%M:%S"))


def log_mean_exp(data):
    """
    Return log(mean(exp(data))) where mean is taken over the last dimension.
    Do not forget about computational stability!
    Using torch.logsumexp is prohibited!
    Input: data, Tensor of shape n_1 x n_2 x ... x n_K.
    Return: Tensor of shape n_1 x n_2 x ,,, x n_{K - 1}.
    """
    c = torch.log(torch.tensor(data.shape[-1]))
    max_val, _ = data.max(dim=-1)
    norm_data = data - torch.unsqueeze(max_val, -1)
    res = max_val + torch.log(torch.sum(torch.exp(norm_data), dim=-1)) - c
    return res

def gaussian_log_pdf(mu, sigma, samples):
    """
    Compute log-likelihood of samples for a given Gaussians with diagonal covariance matrices.
    Input: mu, Tensor of shape n x d - mean vectors for n Gaussians.
    Input: sigma, Tensor of shape n x d - standard deviation vectors for n Gaussians.
    Input: samples, Tensor of shape n x K x d.
    Return: Tensor of shape n x K - element (i, j) is log-likelihood of (i, j)-th sample
            w. r. t. i-th Gaussian.
    """
    mu = torch.unsqueeze(mu, 1)
    sigma = torch.unsqueeze(sigma, 1)
    res = -1 * (samples - mu) ** 2 / (2 * sigma ** 2)
    res = res - torch.log(sigma) - 0.5 * torch.log(torch.tensor(2 * torch.pi))
    return res.sum(axis=list(range(2, len(samples.shape))))

def compute_log_likelihood_monte_carlo(batch, model, generative_log_likelihood, K):
    """
    Monte-Carlo log-likelihood estimation for a batch.
    Log-likelihood must be averaged over all objects in the batch.
    Do not forget to convert the result into float! Otherwise the average of such results forms
    the computational graph stored in memory, which naturaly results in memory overflow soon enough.
    Input: batch, Tensor of shape n x D for VAE or pair of Tensors for CVAE
    Input: model, Module - object with methods prior_distr, sample_latent and generative_distr,
           described in VAE class.
    Input: generative_log_likelihood, function which takes batch and distribution parameters
           produced by the generative network.
    Input: K, int - number of latent samples.
    Return: float - average log-likelihood estimate for the batch.
    """
    with torch.no_grad():
        mu_prior, sigma_prior = model.prior_distr(batch) # each (n, d)
        z_samples = model.sample_latent(mu_prior, sigma_prior, K)
        x_samples = model.generative_distr(z_samples)
        ll = generative_log_likelihood(batch, x_samples) # (n, K)
        ll_obj = log_mean_exp(ll) # (n)

        return ll_obj.mean().item()


def compute_log_likelihood_iwae(batch, model, K):
    """
    IWAE log-likelihood estimation for a batch.
    Log-likelihood must be averaged over all objects in the batch.
    Do not forget to convert the result into float! Otherwise the average of such results forms
    the computational graph stored in memory, which naturaly results in memory overflow soon enough.
    Input: batch, Tensor of shape n x D for VAE or pair of Tensors for CVAE
    Input: model, Module - object with methods prior_distr, proposal_distr, sample_latent
           and generative_distr, described in VAE class.
    Input: generative_log_likelihood, function which takes batch and distribution parameters
           produced by the generative network.
    Input: K, int - number of latent samples.
    Return: float - average log-likelihood estimate for the batch.
    """
    device = next(model.parameters()).device
    with torch.no_grad():
        mu, sigma = model.proposal_distr(batch) # each (n, d)
        mu_prior, sigma_prior = torch.tensor([0.0], device=device), torch.tensor([1.0], device=device)
        z_samples = model.sample_latent(mu, sigma, K)
        x_samples = model.generative_distr(z_samples)

        px_z = model.reconstruction_log_likelihood(batch, x_samples) # (n, K)
        qz_x = gaussian_log_pdf(mu, sigma, z_samples)
        pz = gaussian_log_pdf(mu_prior, sigma_prior, z_samples) # prior

        ll = px_z + pz - qz_x
        ll_obj = log_mean_exp(ll) # (n)

        return ll_obj.mean().item()
    



def save_grid(imgs, path):
    if not isinstance(imgs, list):
        imgs = [imgs]
    fig, axs = plt.subplots(ncols=len(imgs), squeeze=False)
    for i, img in enumerate(imgs):
        img = img.detach()
        img = tF.to_pil_image(img)
        axs[0, i].imshow(np.asarray(img))
        axs[0, i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])
    fig.savefig(path, bbox_inches='tight') 


def print_stats(batch):
    print("Stats:")
    print(f"Shape: {batch.shape}")
    print(f"Mean for channel 1: {batch[:, 0, :, :].mean():.4f}")
    print(f"Mean for channel 2: {batch[:, 1, :, :].mean():.4f}")
    print(f"Mean for channel 3: {batch[:, 2, :, :].mean():.4f}")
    print(f"Std for channel 1: {batch[:, 0, :, :].std():.4f}")
    print(f"Std for channel 2: {batch[:, 1, :, :].std():.4f}")
    print(f"Std for channel 3: {batch[:, 2, :, :].std():.4f}")
    print(f"Min for channel 1: {batch[:, 0, :, :].min():.4f}, Max for channel 1: {batch[:, 0, :, :].max():.4f}")
    print(f"Min for channel 2: {batch[:, 1, :, :].min():.4f}, Max for channel 2: {batch[:, 1, :, :].max():.4f}")
    print(f"Min for channel 3: {batch[:, 2, :, :].min():.4f}, Max for channel 3: {batch[:, 2, :, :].max():.4f}")